{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab1(saud.version).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6SjKoWrROLik","colab_type":"text"},"source":["# **ECE420 - Assignment 1**\n","### Saud Badar - 1002554595\n","### Kyu Bum Kim - 1003969100"]},{"cell_type":"code","metadata":{"id":"BYqcIb2y9mNM","colab_type":"code","outputId":"7aacc9cc-998d-45bf-d1ee-46e6f8c789ef","executionInfo":{"status":"ok","timestamp":1580942915383,"user_tz":300,"elapsed":169,"user":{"displayName":"Saud Badar","photoUrl":"","userId":"01168232222408689448"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import time\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/ECE421/Lab1')\n","\n","def loadData():\n","    with np.load('notMNIST.npz') as data :\n","        Data, Target = data ['images'], data['labels']\n","        posClass = 2\n","        negClass = 9\n","        dataIndx = (Target==posClass) + (Target==negClass)\n","        Data = Data[dataIndx]/255.\n","        Target = Target[dataIndx].reshape(-1, 1)\n","        Target[Target==posClass] = 1\n","        Target[Target==negClass] = 0\n","        np.random.seed(421)\n","        randIndx = np.arange(len(Data))\n","        np.random.shuffle(randIndx)\n","        Data, Target = Data[randIndx], Target[randIndx]\n","        trainData, trainTarget = Data[:3500], Target[:3500]\n","        validData, validTarget = Data[3500:3600], Target[3500:3600]\n","        testData, testTarget = Data[3600:], Target[3600:]\n","    return trainData, validData, testData, trainTarget, validTarget, testTarget\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6775NeiGO0Tv","colab_type":"text"},"source":["# **Section 1: Linear Regression [18 points]**\n","## **1) Loss Function and Gradient [4pt]**\n","Implement two vectorized Numpy functions to compute the loss and gradient respectively. Both functions should accept 5 arguments - the weight vector, the bias, the data matrix, the labels, and λ, the regularization parameter. The loss function return a number (indicating total loss). The gradient function should be an analytical expression of the loss function and return the gradient with respect to the weights and the gradient with respect to the bias. Both function headers are below. Include both the analytical expression for the gradient and\n","the Python code snippet in your report.\n","\n","### Answer: "]},{"cell_type":"code","metadata":{"id":"ae6jKDqr90DJ","colab_type":"code","colab":{}},"source":["\"\"\"\n","calculates the mean squared error and returns the total loss\n","W is a weight, 1D-array of size: 784 (28 x 28) \n","b is the bias\n","x is the batch of Mnist data images : (3500, 28, 28)\n","y is the batch of labels : (3500, 1)\n","reg is λ, the regularization parameter\n","\"\"\"\n","def MSE(W, b, x, y, reg):\n","  #reshaping the batch to satisfy the size of weights for multiplication \n","  #(x shape = (3500, 784), W shape = (784, 1))\n","  x_flat = np.reshape(x, (x.shape[0], x.shape[1] * x.shape[2]))\n"," \n","  #calculate error using the formula (e shape = (3500, 1))\n","  e = np.subtract(np.matmul(x_flat, W) + b, y)\n"," \n","  #calculating overall mean squared error (loss function)\n","  mse = np.square(e).mean()\n"," \n","  #calculating weight decay loss\n","  weight_loss = reg / 2 * (np.linalg.norm(W) ** 2)\n","  \n","  #return the sum of the two losses for overall loss\n","  return weight_loss + mse\n"," "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TwoW6yU91Sg","colab_type":"code","colab":{}},"source":["def gradMSE(W, b, x, y, reg):\n","  # y = y.squeeze()\n","  #   pred = np.matmul(x,W) + b\n","  #   err = y - pred\n","    \n","  #   #second part of this multiplies each error value for each input\n","  #   df_dw = -2*np.matmul(x.transpose(), err)/len(x) + reg*W\n","  #   df_db = np.mean(-2*err)\n","\n","  #   return df_dw, df_db\n","    # Your implementation here\n","    # Reshaping part of the code \n","    newX = np.reshape(x, (x.shape[0], x.shape[1] * x.shape[2])) # New shape of x is [3500, 784]\n","\n","    #calculate error using the formula (e shape = (3500, 1))\n","    e_in = np.subtract(np.matmul(newX, newW) + b, y)\n","\n","    #Find the gradient of weights\n","    w_gradient = (2 * np.matmul(newX.transpose(), e_in))/(x.shape[0])\n","\n","    #Add the regulization factor\n","    w_gradient = w_gradient + reg*newW \n","\n","    #Find the gradient of the bias\n","    b_gradient = np.sum(e_in) / (x.shape[0])\n","\n","    return w_gradient, b_gradient\n","   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5HEtaB7nRVTI","colab_type":"text"},"source":["## **2) Gradient Descent Implementation [6 pts]:**\n","Using the gradient computed from Part A, implement the batch Gradient Descent algorithm\n","to classify the two classes in the notMNIST dataset. The function should accept 8 arguments\n","the weight vector, the bias, the data matrix, the labels, the learning rate, the number of\n","epochs 1 , λ and an error tolerance (set to 1 × 10 −7 ). The error tolerance will be used to\n","compute the difference between the old and updated weights per iteration. The function\n","should return the optimized weight vector and bias.\n","### Answer:"]},{"cell_type":"code","metadata":{"id":"HTeCqHlK92co","colab_type":"code","colab":{}},"source":["\"\"\"\n","calculates optimal values of the vectors weights and bias determined by\n","the gradient of mean squared error loss function (MSE) and returns these \n","optimal values \n","returns weight, then bias\n","W is the weight\n","b is the bias\n","x is the data\n","y is the labels\n","alpha is learning rate (step)\n","epochs are iterations for each batch\n","reg is λ, the regularization parameter\n","error_tol is the error tolerance (set to 1 × 10 −7 )\n","lossType is the type of loss function (MSE or CE)\n","\"\"\"\n","def grad_descent(W, b, x, y, alpha, epochs, reg, error_tol, lossType):\n","  trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n","  train_loss = []\n","  test_loss = []\n","  valid_loss = []\n","  train_acc = []\n","  valid_acc = []\n","  test_acc = []\n","\n","  for i in range(epochs):\n","    train_acc.append(accuracy(W, b, x, y, lossType))\n","    valid_acc.append(accuracy(W, b, validData, validTarget, lossType))\n","    test_acc.append(accuracy(W, b, testData, testTarget, lossType))\n","    #find loss and gradient weight and bias determined by input of this function\n","    if lossType == 'MSE':\n","      loss = MSE(W, b, x, y, reg)\n","      train_loss.append(loss)\n","      valid_loss.append(MSE(W, b, validData, validTarget, reg))\n","      test_loss.append(MSE(W, b, testData, testTarget, reg))\n","      weights, bias = gradMSE(W, b, x, y, reg)\n","\n","    elif lossType == 'CE':\n","      loss = crossEntropyLoss(W, b, x, y, reg)\n","      train_loss.append(loss)\n","      valid_loss.append(crossEntropyLoss(W, b, validData, validTarget, reg))\n","      test_loss.append(crossEntropyLoss(W, b, testData, testTarget, reg))\n","\n","      weights, bias = gradCE(W, b, x, y, reg)\n","      \n","      #if error is smaller than the tolerance, we have the optimal values\n","      #for weights and bias, thus we can stop iterating\n","\n","    if loss <= error_tol:\n","      break \n","\n","    #update ewight and bias from calculated gradient\n","    W -= alpha * weights\n","    b -= alpha * bias\n","  return W, b, train_loss, valid_loss, test_loss, train_acc, valid_acc, test_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iR9KgWnZSNtW","colab_type":"text"},"source":["## **3) Tuning the Learning Rate [3 pts]:**\n","\n","Test your implementation of Gradient Descent with 5000 epochs and λ = 0. Investigate the impact of learning rate, α = 0.005, 0.001, 0.0001 on the performance of your classifier. Plot the training, validation and test losses.\n","\n","### Answer:\n","**Learning Rate = 0.005 Graphs**\n","\n","Loss/Error: \n","\n","![Learning_Rate = 0.005 (Error)](https://i.imgur.com/wlJnMsk.jpg)\n","\n","Accuracy:\n","\n","\n","![Learning_Rate = 0.005 (Acc)](https://i.imgur.com/44QTwRT.png)\n","\n","\n","**Learning Rate = 0.001 Graphs**\n","\n","Loss/Error:\n","\n","![Learning_Rate = 0.001 (Error)](https://i.imgur.com/kequ8Lf.png)\n","\n","Accuracy:\n","\n","![Learning_Rate = 0.001 (Acc)](https://i.imgur.com/WuMUKhn.png)\n","\n","**Learning Rate = 0.0001 Graphs**\n","\n","Loss/Error:\n","\n","![Learning_Rate = 0.0001 (Error)](https://i.imgur.com/7epr35j.png)\n","\n","Accuracy:\n","\n","![Learning_Rate = 0.0001 (Acc)](https://i.imgur.com/WVkYuXj.png)\n","\n","From what we can tell all the the training, validation and the test loss are converging slowly towards zero. So the higher our learning rate is the faster the graphs converges to zero. We can tell this from the lowest losses in each graph, **0.005 is 0.0265, 0.001 is 0.0407, 0.0001 is 0.0611.** This shows the higher the learning rate is faster it converges to zero. We have to keep in mind not too increase the learning curve too much as that will cause our graph to never converge."]},{"cell_type":"markdown","metadata":{"id":"M_xqX8RxsqjR","colab_type":"text"},"source":["## **4) Generalization [3 pts]:**\n","\n","Investigate impact by modifying the regularization parameter, λ = {0.001, 0.1, 0.5}. Plot the training, validation and test loss for α = 0.005 and report the final training, validation and test accuracy of your classifier.\n","\n","### Answer:\n","\n","**Regularization Parameter = 0.001**\n","\n","Error:\n","\n","![Regularization Parameter = 0.001 (Error)](https://i.imgur.com/LaVoRvu.png)\n","\n","Accuracy:\n","\n","![Regularization Parameter = 0.001 (Acc)](https://i.imgur.com/gZioAPA.png)\n","\n","Final Training Accuracy: 0.986, Final Validation Accuracy: 1.0, Final Test Accuracy: 0.97931 \n","\n","\n","**Regularization Parameter = 0.1**\n","\n","Error:\n","\n","![Regularization Parameter = 0.1 (Error)](https://i.imgur.com/CGclalh.png)\n","\n","\n","Accuracy:\n","\n","![Regularization Parameter = 0.1 (Acc)](https://i.imgur.com/78yq5Xb.png)\n","\n","Final Training Accuracy: 0.98428, Final Validation Accuracy: 1.0, Final Test Accuracy: 0.97931 \n","\n","**Regularization Parameter = 0.5**\n","\n","Error:\n","\n","![Regularization Parameter = 0.5 (Error)](https://i.imgur.com/TCwwK9X.png)\n","\n","\n","Accuracy:\n","\n","![Regularization Parameter = 0.5 (Acc)](https://i.imgur.com/GaFJftJ.png)\n","\n","Final Training Accuracy: 0.98057, Final Validation Accuracy: 1.0, Final Test Accuracy: 0.9862"]},{"cell_type":"markdown","metadata":{"id":"04a-ZkzhuB1M","colab_type":"text"},"source":["## **5) Comparing Batch GD with normal equation [2 pts]:**\n","\n","For linear regression, you can find the optimum weights using the closed form equation for the derivative of the means square error (normal equation). For zero weight decay, Write a Numpy script to find the optimal linear regression weights on the two-class notMNIST dataset using the ”normal equation” of the least squares formula. Compare in terms of final training MSE, accuracy and computation time between Batch GD and normal equation.\n","\n","### Answer: "]},{"cell_type":"code","metadata":{"id":"KvO8IBySHQ9n","colab_type":"code","colab":{}},"source":["\"\"\"\n","x is the data\n","y is the labels\n","\"\"\"\n","def leastSquares(x, y, b, reg):\n","  # Start the timer\n","  start = time.time()\n"," \n","  # Reshape the matrix and find the transpose of the new matrix\n","  x_Orig = x.reshape((x.shape[0], x.shape[1] * x.shape[2]))\n"," \n","  # Add the one bais vector to all the weights\n","  xTranspose = x_Orig.transpose()\n"," \n","  # Calculate the the error using thid formula W = [(X.T * X)^-1 *(X.T * Y)] \n","  e_in = np.linalg.inv(np.matmul(xTranspose, x_Orig))\n","  wStar = np.matmul(e_in, np.matmul(xTranspose, y))\n","  b = wStar[0][0]\n"," \n","  # End the timer\n","  end = time.time()\n","  totalTime = (end - start)\n","  \n","  # Find the Accuracy \n","  accuracyLeastSquares = accuracy(wStar, b, x, y, 'MSE')\n","  print(\"This is the accuracy of the normal equation: \",accuracyLeastSquares)\n","  print(\"This is the time it took to complete this computation: %.3lfs\" % totalTime)\n"," \n","  return wStar, b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NuZ3j-wYUNzB","colab_type":"code","colab":{}},"source":["\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lW17CWk2tV9d","colab_type":"code","colab":{}},"source":["\"\"\"\n","HELPER FUNCTIONS\n","\"\"\"\n","\n","def sigmoid(x):\n","  return 1 / (1 + np.exp(-x))\n","\n","def plot (W, b, alpha, epochs, reg, lossType):\n","  trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n","  n = epochs # number of epochs\n","  error_tol = 10**-7\n","  W, b, train_loss_arr, valid_loss_arr, test_loss_arr, train_acc, valid_acc, test_acc = grad_descent_V2(W, b, trainData, trainTarget, alpha, epochs, reg, error_tol, lossType)\n","\n","  plt.figure(figsize=(20,10))\n","  plt.title(\"Train, Validation and Test Error (Regularization Parameter = 0.5)\")\n","  print(\"This is the lowest Train loss: \", np.amin(train_loss_arr))\n","  print(\"This is the lowest Validation loss: \", np.amin(valid_loss_arr))\n","  print(\"This is the lowest Test loss: \", np.amin(test_loss_arr))\n","  plt.plot(range(n), train_loss_arr, label = \"Training Set\")\n","  plt.plot(range(n), valid_loss_arr, label = \"Validation Set\")\n","  plt.plot(range(n), test_loss_arr, label = \"Test Set\")\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Error\")\n","  plt.legend(loc='best')\n","  plt.show()\n","\n","  plt.figure(figsize=(20,10))\n","  plt.title(\"Train, Validation and Test Accuracy (Regularization Parameter = 0.5)\")\n","  print(\"This is the lowest Train accuracy: \", np.amin(train_acc))\n","  print(\"This is the lowest Validation accuracy: \", np.amin(valid_acc))\n","  print(\"This is the lowest Test accuracy: \", np.amin(test_acc))\n","  print(\"This is the maximum Train accuracy: \", np.amax(train_acc))\n","  print(\"This is the maximum Validation accuracy: \", np.amax(valid_acc))\n","  print(\"This is the maximum Test accuracy: \", np.amax(test_acc))\n","  print(\"This is the final Train accuracy: \", train_acc[epochs - 1])\n","  print(\"This is the final Validation accuracy: \", valid_acc[epochs - 1])\n","  print(\"This is the final Test accuracy: \", test_acc[epochs - 1])\n","  plt.plot(range(n), train_acc, label = \"Training Set\")\n","  plt.plot(range(n), valid_acc, label = \"Validation Set\")\n","  plt.plot(range(n), test_acc, label = \"Test Set\")\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Accuracy (0 to 1)\")\n","  plt.legend(loc='best')\n","  plt.show()\n","  \n","def accuracy(W, b, x, y, lossType):\n","  #reshaping the batch to satisfy the size of weights for multiplication \n","  #(x shape = (3500, 784), W shape = (784, 1))\n","  x_flat = x.reshape((x.shape[0], x.shape[1] * x.shape[2]))\n"," \n","  #calculate error using the formula (e shape = (3500, 1))\n","  prediction = np.matmul(x_flat, W) + b\n","\n","  if lossType == 'CE':\n","    prediction = sigmoid(prediction)\n","\n","  N = x.shape[0]\n","  correct = 0\n","  for i in range(N):\n","    if (prediction[i] >= 0.5 and y[i] == 1) or (prediction[i] < 0.5 and y[i] == 0):\n","      correct += 1\n","  return correct / N"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"stSsPii6UCQL","colab_type":"text"},"source":["# **Section 2: Logistic Regression [10 points]**\n","\n","## **1) Loss Function and Gradient [4 pts]:**\n","\n","Implement two vectorized Numpy functions to compute the Binary Cross Entropy Error and its gradient respectively. Similar to Part 1.1, both functions should accept 5 arguments - the weight vector, the bias, the data matrix, the labels, and the regularization parameter. They should return the loss and gradients with respect to weights and bias respectively. Include the analytical expressions in your report as well as a snippet of your Python code.\n","\n","### Answer:"]},{"cell_type":"code","metadata":{"id":"l1Ao_YQ893io","colab_type":"code","colab":{}},"source":["\"\"\"\n","calculates the cross entropy and returns the total loss\n","W is a weight, 1D-array of size: 784 (28 x 28) \n","b is the bias\n","x is the batch of Mnist data images : (3500, 28, 28)\n","y is the batch of labels : (3500, 1)\n","reg is λ, the regularization parameter\n","\"\"\"\n","def crossEntropyLoss(W, b, x, y, reg):\n","  # Your implementation here\n","  #reshaping the batch to satisfy the size of weights for multiplication \n","  #(x shape = (3500, 784), W shape = (784, 1))\n","  x_flat = x.reshape((x.shape[0], x.shape[1] * x.shape[2]))\n","\n","  #for sigmoid func (prediction matrix shape = (3500, 1))\n","  prediction_matrix = np.dot(x_flat, W) + b\n","  \n","  #obtain model output (finding values between 1 and 0)\n","  model_output = sigmoid(prediction_matrix)\n","\n","  #calculate error using the formula \n","  left_term = np.multiply(-1*y, np.log(model_output))\n","  right_term = np.multiply(1 - y, np.log(1 - model_output))\n","  ce = (left_term - right_term).mean()\n","\n","  #calculating weight decay loss\n","  weight_loss = (reg / 2) * (np.linalg.norm(W) ** 2)\n","\n","  #return the sum of the two losses for overall loss\n","  return weight_loss + ce"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UsiBZCqX9445","colab_type":"code","colab":{}},"source":["def gradCE(W, b, x, y, reg):\n","    # Your implementation here\n","    # Reshaping part of the code \n","    newW = W.reshape((x.shape[1] * x.shape[2], 1))  # New shape of W is [784, 1]\n","    newX = x.reshape((x.shape[0], x.shape[1] * x.shape[2])) # New shape of x is [3500, 784]\n","\n","    #Version 2\n","    length = x.shape[0]\n","\n","    #Find our prediction\n","    x_Prediction = np.dot(newX, newW) + b\n","\n","    #Obtain model output (finding values between 1 and 0)\n","    model_output = sigmoid(x_Prediction)\n","    e_in = model_output - y\n","\n","    #Find the gradient with respect to the weight\n","    g_Gradient = (np.dot(newX.transpose(),  e_in))/length\n","\n","    #Add the regularization factor\n","    g_Gradient = g_Gradient + reg * newW \n","\n","    #Find the gradient with respect to the bias\n","    b_Gradient = (np.sum(e_in))/ length\n","\n","    return (g_Gradient, b_Gradient)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"04f-VAoRnTcV","colab_type":"text"},"source":["## **2) Learning [4 pts]:**\n","Modify the function from Part 1.2 to include a flag, specifying the type of loss/gradient to use in the classifier. Modify your function to update weights and bias using the Binary Cross Entropy loss and report on the performance of the Logistic Regression model by setting regularization parameter = 0.1, learning curve = 0.005, and 5000 epochs. Plot the loss and accuracy curves for training, validation, and test data set.\n","\n","### Answer:\n","We already implemented this feture in Section 2 Part 2. We added a commented version of that code in the section down below."]},{"cell_type":"code","metadata":{"id":"HWSrr0cioXA0","colab_type":"code","colab":{}},"source":["\"\"\"\n","def grad_descent(W, b, x, y, alpha, epochs, reg, error_tol, lossType):\n","  trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n","  train_loss = []\n","  test_loss = []\n","  valid_loss = []\n","  train_acc = []\n","  valid_acc = []\n","  test_acc = []\n","\n","  for i in range(epochs):\n","    train_acc.append(accuracy(W, b, x, y, lossType))\n","    valid_acc.append(accuracy(W, b, validData, validTarget, lossType))\n","    test_acc.append(accuracy(W, b, testData, testTarget, lossType))\n","    #find loss and gradient weight and bias determined by input of this function\n","    if lossType == 'MSE':\n","      loss = MSE(W, b, x, y, reg)\n","      train_loss.append(loss)\n","      valid_loss.append(MSE(W, b, validData, validTarget, reg))\n","      test_loss.append(MSE(W, b, testData, testTarget, reg))\n","      weights, bias = gradMSE(W, b, x, y, reg)\n","\n","    elif lossType == 'CE':\n","      loss = crossEntropyLoss(W, b, x, y, reg)\n","      train_loss.append(loss)\n","      valid_loss.append(crossEntropyLoss(W, b, validData, validTarget, reg))\n","      test_loss.append(crossEntropyLoss(W, b, testData, testTarget, reg))\n","\n","      weights, bias = gradCE(W, b, x, y, reg)\n","      \n","      #if error is smaller than the tolerance, we have the optimal values\n","      #for weights and bias, thus we can stop iterating\n","\n","    if loss <= error_tol:\n","      break \n","\n","    #update ewight and bias from calculated gradient\n","    W -= alpha * weights\n","    b -= alpha * bias\n","  return W, b, train_loss, valid_loss, test_loss, train_acc, valid_acc, test_acc\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YsWRuSkapM0d","colab_type":"text"},"source":["## **3) Comparison to Linear Regression [2 pts]:**\n","For zero weight decay, learning rate of 0.005 and 5000 epochs , plot the training cross entropy loss and MSE loss for logistic regression and linear regression respectively. Comment on the effect of cross-entropy loss convergence behaviour.\n","\n","### Answer:\n","From the loss function of cross-entropy we can easily tell that the logistic regression model using cross-entropy converges faster than linear regression model using MSE. This tells us that cross-entropy is a better at measuring the loss compared to MSE for classification problems. In our case just by looking at the graph we can tell this does not happen.\n"]},{"cell_type":"code","metadata":{"id":"eeeYI7-196EJ","colab_type":"code","colab":{}},"source":["def buildGraph(loss, b1, b2, eps):\n","\t#Initialize weight and bias tensors\n","\ttf.set_random_seed(421)\n","\tinput_size = 28*28\n","\tW = tf.Variable(tf.truncated_normal(stddev=0.5, shape=(input_size, 1), dtype=tf.float32))\n","\tb = tf.Variable(name='bias', initial_value = tf.ones(1), dtype=tf.float32)\n","\tx = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, input_size))\n","\ty = tf.compat.v1.placeholder(dtype=tf.int8, shape=(None))\n","\treg = tf.placeholder(dtype=tf.float32, name='lambda')\n","\n","\tpredicted_y = tf.add(tf.matmul(x, W), b)\n","\toptimizer = tf.train.AdamOptimizer(learning_rate = 0.001, beta1=b1, beta2=b2, epsilon=eps)\n","\n","\tif loss == \"MSE\":\n","\t# Your implementation\n","\t\terr = tf.losses.mean_squared_error(y, predicted_y)\n","\t\t\n","\telif loss == \"CE\":\n","\t#Your implementation here\n","\t\tpredicted_y = tf.sigmoid(predicted_y)\n","\t\terr = tf.losses.sigmoid_cross_entropy(y, predicted_y)\n","\t\n","\toptimal = optimizer.minimize(err)\n","\treturn W, b, x, predicted_y, y, err, optimal\n","\n","\t'''\n","W is the weight\n","b is the bias\n","x is the data\n","y is the labels\n","alpha is learning rate (step)\n","epochs are iterations for each batch\n","reg is λ, the regularization parameter\n","error_tol is the error tolerance (set to 1 × 10 −7 )\n","lossType is the type of loss function (MSE or CE)\n","'''\n","def SGD(alpha, epochs, reg, error_tol, batch_size, lossType, b1, b2, eps):\n","  trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n","  train_loss_arr = []\n","  train_acc_arr = []\n","  valid_loss_arr = []\n","  valid_acc_arr = []\n","  test_loss_arr = []\n","  test_acc_arr = []\n","  train_flat = trainData.reshape((trainData.shape[0], trainData.shape[1] * trainData.shape[2]))\n","  valid_flat = validData.reshape((validData.shape[0], validData.shape[1] * validData.shape[2]))\n","  test_flat = testData.reshape((testData.shape[0], testData.shape[1] * testData.shape[2]))\n","  weights, bias, x, predicted_y, y, error, optimal = buildGraph(lossType, b1, b2, eps)\n","  with tf.Session() as session:\n","    session.run(tf.initialize_all_variables())\n","    for step in range(epochs):\n","      batches = shuffle(train_flat, trainTarget, batch_size)\n","      \n","      for batch in enumerate(batches):\n","        train_data = batch[1][:,0:784]\n","        train_label = batch[1][:,784]\n","        train_label = np.expand_dims(train_label, axis=1)\n","        session.run(optimal, feed_dict = {x: train_data, y: train_label})\n","      \n","      train_loss = session.run(error, feed_dict={x: train_data, y: train_label})\n","      valid_loss = session.run(error, feed_dict={x: valid_flat, y: validTarget})\n","      test_loss = session.run(error, feed_dict={x: test_flat, y: testTarget})\n","\n","      train_loss_arr.append(train_loss)\n","      valid_loss_arr.append(valid_loss)\n","      test_loss_arr.append(test_loss)\n","      \n","      train_pred = session.run(predicted_y, feed_dict={x: train_data, y: train_label})\n","      valid_pred = session.run(predicted_y, feed_dict={x: valid_flat, y: validTarget})\n","      test_pred = session.run(predicted_y, feed_dict={x: test_flat, y: testTarget})\n","\n","      train_acc_arr.append(accuracy_given_pred(train_pred, train_label))\n","      valid_acc_arr.append(accuracy_given_pred(valid_pred, validTarget))\n","      test_acc_arr.append(accuracy_given_pred(test_pred, testTarget))\n","  \n","  return train_loss_arr, train_acc_arr, valid_loss_arr, valid_acc_arr, test_loss_arr, test_acc_arr\n","\n","\n","def accuracy_given_pred(predicted, expected):\n","  N = predicted.shape[0]\n","  correct = 0\n","  for i in range(N):\n","    if (predicted[i] >= 0.5 and expected[i] == 1) or (predicted[i] < 0.5 and expected[i] == 0):\n","      correct += 1\n","  #print(correct/N)\n","  return correct / N\n","\n","\n","'''\n","shuffles the dataset into a random order for SGD\n","'''\n","def shuffle(data, labels, batch_size):\n","  #join the data and labels together\n","  data_with_labels = np.concatenate((data, labels), axis=1)\n","\n","  #shuffle them\n","  np.random.shuffle(data_with_labels)\n","\n","  #split each one by batch size\n","  randomized_data_with_labels = np.array_split(data_with_labels, data_with_labels.shape[0] // batch_size, axis = 0)\n","\n","  #return each one\n","  return randomized_data_with_labels\n","\n","def plot_sgd (train_loss_arr, train_acc, valid_loss_arr, valid_acc, test_loss_arr, test_acc, lossType, info):\n","\n","  n = len(train_loss_arr)\n","  # plt.figure(figsize=(20,10))\n","  # plt.title('SGD ' + lossType + ' Loss (' + info + ')')\n","  # print(\"This is the lowest Train loss: \", np.amin(train_loss_arr))\n","  # print(\"This is the lowest Validation loss: \", np.amin(valid_loss_arr))\n","  # print(\"This is the lowest Test loss: \", np.amin(test_loss_arr))\n","  # plt.plot(range(n), train_loss_arr, label = \"Training Set\")\n","  # plt.plot(range(n), valid_loss_arr, label = \"Validation Set\")\n","  # plt.plot(range(n), test_loss_arr, label = \"Test Set\")\n","  # plt.xlabel(\"Epoch\")\n","  # plt.ylabel(\"Error\")\n","  # plt.legend(loc='best')\n","  # plt.show()\n","\n","  # plt.figure(figsize=(20,10))\n","  # plt.title(\"SGD \" + lossType + ' Accuracy (' + info  + ')')\n","  # print(\"This is the lowest Train accuracy: \", np.amin(train_acc))\n","  # print(\"This is the lowest Validation accuracy: \", np.amin(valid_acc))\n","  # print(\"This is the lowest Test accuracy: \", np.amin(test_acc))\n","  # print(\"This is the maximum Train accuracy: \", np.amax(train_acc))\n","  # print(\"This is the maximum Validation accuracy: \", np.amax(valid_acc))\n","  # print(\"This is the maximum Test accuracy: \", np.amax(test_acc))\n","  print(\"This is the final Train accuracy: \", train_acc[n - 1])\n","  print(\"This is the final Validation accuracy: \", valid_acc[n -1])\n","  print(\"This is the final Test accuracy: \", test_acc[n - 1])\n","  # plt.plot(range(n), train_acc, label = \"Training Set\")\n","  # plt.plot(range(n), valid_acc, label = \"Validation Set\")\n","  # plt.plot(range(n), test_acc, label = \"Test Set\")\n","  # plt.xlabel(\"Epoch\")\n","  # plt.ylabel(\"Accuracy (0 to 1)\")\n","  # plt.legend(loc='best')\n","  # plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"INWGuN5L6dRW","colab_type":"code","outputId":"fb5ba0d8-9fcd-4c51-9c60-b60f12071070","executionInfo":{"status":"ok","timestamp":1580948373736,"user_tz":300,"elapsed":29043,"user":{"displayName":"Saud Badar","photoUrl":"","userId":"01168232222408689448"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["\"\"\"\n","TEST\n","\"\"\"\n","W = np.zeros((28*28, 1))\n","b = 1\n","reg = 0\n","alpha = 0.001\n","epochs = 700 # number of epochs\n","error_tol = 10**-7\n","lossType = 'CE'\n","batch_size = 1750\n","b1=0.95\n","b2=0.999\n","eps=10**-8\n","# Testing Grad Descent\n","train_loss_arr, train_acc_arr, valid_loss_arr, valid_acc_arr, test_loss_arr, test_acc_arr = SGD(alpha, epochs, reg, error_tol, batch_size, lossType, b1, b2, eps)\n","plot_sgd(train_loss_arr, train_acc_arr, valid_loss_arr, valid_acc_arr, test_loss_arr, test_acc_arr, lossType, 'Experiment 1: Beta 1 = 0.95 (MSE)')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["This is the final Train accuracy:  0.9845714285714285\n","This is the final Validation accuracy:  0.97\n","This is the final Test accuracy:  0.9862068965517241\n"],"name":"stdout"}]}]}