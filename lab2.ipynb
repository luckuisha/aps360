{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"IE1_FHVaUBsm","colab_type":"text"},"source":["# **ECE420 - Assignment 2**\n","### Saud Badar - 1002554595\n","### Kyu Bum Kim - 1003969100"]},{"cell_type":"code","metadata":{"id":"l5Y8-3qZPwMy","colab_type":"code","outputId":"6a24ed86-fa42-44a7-f968-97d45a43fb5b","executionInfo":{"status":"ok","timestamp":1583450623589,"user_tz":300,"elapsed":24662,"user":{"displayName":"Saud Badar","photoUrl":"","userId":"01168232222408689448"}},"colab":{"base_uri":"https://localhost:8080/","height":171}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/ECE421/Lab2')"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FEkJ8lR5QwX9","colab_type":"code","colab":{}},"source":["# Load the data\n","def loadData():\n","    with np.load(\"notMNIST.npz\") as data:\n","        Data, Target = data[\"images\"], data[\"labels\"]\n","        np.random.seed(521)\n","        randIndx = np.arange(len(Data))\n","        np.random.shuffle(randIndx)\n","        Data = Data[randIndx] / 255.0\n","        Target = Target[randIndx]\n","        trainData, trainTarget = Data[:10000], Target[:10000]\n","        validData, validTarget = Data[10000:16000], Target[10000:16000]\n","        testData, testTarget = Data[16000:], Target[16000:]\n","    return trainData, validData, testData, trainTarget, validTarget, testTarget"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m74DgztmReap","colab_type":"code","colab":{}},"source":["def convertOneHot(trainTarget, validTarget, testTarget):\n","    newtrain = np.zeros((trainTarget.shape[0], 10))\n","    newvalid = np.zeros((validTarget.shape[0], 10))\n","    newtest = np.zeros((testTarget.shape[0], 10))\n","\n","    for item in range(0, trainTarget.shape[0]):\n","        newtrain[item][trainTarget[item]] = 1\n","    for item in range(0, validTarget.shape[0]):\n","        newvalid[item][validTarget[item]] = 1\n","    for item in range(0, testTarget.shape[0]):\n","        newtest[item][testTarget[item]] = 1\n","    return newtrain, newvalid, newtest"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5aZanEZojN6I","colab_type":"code","colab":{}},"source":["def shuffle(trainData, trainTarget):\n","    np.random.seed(421)\n","    randIndx = np.arange(len(trainData))\n","    target = trainTarget\n","    np.random.shuffle(randIndx)\n","    data, target = trainData[randIndx], target[randIndx]\n","    return data, target"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNqdaE0hxCIl","colab_type":"code","outputId":"b201e19e-4e8a-4e31-bf9a-efe9cb017ce0","executionInfo":{"status":"ok","timestamp":1583437524941,"user_tz":300,"elapsed":675,"user":{"displayName":"Saud Badar","photoUrl":"","userId":"01168232222408689448"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["tf.reset_default_graph()\n","x = tf.placeholder(\"float\", [None, 28,28,1])\n","true = tf.placeholder(\"float\", [None, 10])\n","\n","# This is the first pair of weights and biases\n","with tf.variable_scope(\"Weight_11\", reuse = tf.AUTO_REUSE):\n","  weight_in = tf.get_variable('Weight_11', shape = (3, 3, 1, 32), initializer = tf.contrib.layers.xavier_initializer())\n","with tf.variable_scope(\"Bias_11\", reuse = tf.AUTO_REUSE):\n","  bias_in =  tf.get_variable('Bias_11', shape = (32), initializer = tf.contrib.layers.xavier_initializer())\n","\n","#This is the second pair of weights and biases for the weights between conv2d and fc1\n","with tf.variable_scope(\"Weight_22\", reuse = tf.AUTO_REUSE):\n","  weight_fc1 = tf.get_variable('Weight_22', shape = (14*14*32, 784), initializer = tf.contrib.layers.xavier_initializer())\n","with tf.variable_scope(\"Bias_22\", reuse = tf.AUTO_REUSE):\n","  bias_fc1 = tf.get_variable('Bias_22', shape = (784), initializer = tf.contrib.layers.xavier_initializer())\n","\n","#This is the third pair of weights and biases for the weights between fc1 and fc2\n","with tf.variable_scope(\"Weight_33\", reuse = tf.AUTO_REUSE):\n","  weight_fc2 = tf.get_variable('Weight_33', shape = (784, 10), initializer = tf.contrib.layers.xavier_initializer())\n","with tf.variable_scope(\"Bias_33\", reuse = tf.AUTO_REUSE):\n","  bias_fc2 = tf.get_variable('Bias_33', shape = (10), initializer = tf.contrib.layers.xavier_initializer())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u9OTnSIg01Og","colab_type":"code","colab":{}},"source":["def conv2d(x, W, b):\n","    x = tf.nn.conv2d(x, W, strides =[1, 1, 1, 1], padding='SAME')\n","    x = tf.nn.bias_add(x, b)\n","    return (x)\n","\n","def maxpool2d(x, size):\n","    x = tf.nn.max_pool(x, ksize=[1, size, size, 1], strides=[1, size, size, 1], padding='SAME')\n","    return(x)\n","\n","trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n","trainData = trainData.reshape(-1, 28, 28, 1)\n","validData = validData.reshape(-1, 28, 28, 1)\n","testData = testData.reshape(-1, 28, 28, 1)\n","trainTarget, validTarget, testTarget = convertOneHot(trainTarget, validTarget, testTarget)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJLyggZOw-fz","colab_type":"code","colab":{}},"source":["learning_rate = 1e-4\n","batch_size = 32\n","epochs = 50\n","reg = 0.5\n","drop = 0.5\n","\n","def CNN(x):\n","    #1. Input Layer(This has already been created in the code above)\n","\n","    #2.A 3 × 3 convolutional layer, with 32 filters, using vertical and horizontal strides of 1.\n","    #(use my own fuction)\n","    convolutional_layer =  conv2d(x, weight_in, bias_in)\n","\n","    #3. ReLU activation\n","    relu_layer = tf.nn.relu(convolutional_layer)\n","\n","    #4. A batch normalization layer\n","    #4.1 Get the mean and varience\n","    batchMean, batchVar = tf.nn.moments(relu_layer, [0])\n","    #4.2 Normalizing the data\n","    normal_layer = tf.nn.batch_normalization(relu_layer, mean = batchMean, variance= batchVar, offset = 0, scale = None, variance_epsilon= 1e-8)\n","\n","    #5. A 2 × 2 max pooling layer (use my own fuction)\n","    max_pool_layer = maxpool2d(normal_layer, 2)\n","\n","    #6. Flatten layer\n","    flatten_layer = tf.reshape(max_pool_layer, [-1, weight_fc1.get_shape().as_list()[0]])\n","\n","    #7. Fully connected layer (with 784 output units, i.e. corresponding to each pixel)\n","    fully_connected_1_unbiased = tf.matmul(flatten_layer, weight_fc1)\n","    fully_connected_one =  tf.add(fully_connected_1_unbiased, bias_fc1)\n","\n","    #8. ReLU activation\n","    relu_layer_two = tf.nn.relu(fully_connected_one)\n","\n","    #8.1 The hidden dropout layer\n","    if drop != 0:\n","      relu_layer_two = tf.nn.dropout(relu_layer_two, rate = drop)\n","\n","    #9. Fully connected layer (with 10 output units, i.e. corresponding to each class)\n","    fully_connected_2_unbiased = tf.matmul(relu_layer_two, weight_fc2)\n","    out =  tf.add(fully_connected_2_unbiased, bias_fc2)\n","\n","    return (out)\n","\n","# In this step we get the value of \n","output = CNN(x)\n","\n","#10. Softmax output (For accuracy)\n","softmax = tf.nn.softmax(output)\n","\n","#11. Cross Entropy loss\n","cost_CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels= true, logits = output))\n","\n","# regularizers = tf.nn.l2_loss(weight_in) + tf.nn.l2_loss(weight_fc1) + tf.nn.l2_loss(weight_fc2)\n","# cost_CE = tf.reduce_mean(cost_CE + reg * regularizers)\n","\n","optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_CE)\n","\n","#Prediction Calculation\n","compared_prediction = tf.equal(tf.argmax(softmax, 1), tf.argmax(true, 1))\n","\n","#Accuracy Calculation\n","final_acc = tf.reduce_mean(tf.cast(compared_prediction, tf.float64))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I4tXAA3W2caz","colab_type":"code","colab":{}},"source":["train_loss, val_loss, test_loss = [], [], []\n","train_acc, val_acc, test_acc = [], [], []\n","\n","\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","\n","    for i in range(epochs):\n","        batch_loop = len(trainData)//batch_size\n","        #Second for loop for the batch size\n","        for j in range(batch_loop):\n","            batch_start = j * batch_size\n","            batch_end = min(((j + 1) * batch_size), len(trainData))\n","\n","            batch_data = trainData[batch_start:batch_end]\n","            \n","            batch_end = min(((j + 1) * batch_size), len(trainTarget))\n","            batch_target = trainTarget[batch_start: batch_end]\n","            optimize = sess.run(optimizer, feed_dict= {x: batch_data, true: batch_target})\n","            \n","        #Now we will calculate the loss for each iteration\n","        epoch_train_loss = sess.run(cost_CE, feed_dict={x: trainData, true: trainTarget})\n","        epoch_train_acc = sess.run(final_acc, feed_dict ={x: trainData, true: trainTarget})\n","        epoch_val_loss = sess.run(cost_CE, feed_dict={x: validData, true: validTarget})\n","        epoch_val_acc = sess.run(final_acc, feed_dict ={x: validData, true: validTarget})\n","        epoch_test_loss = sess.run(cost_CE, feed_dict={x: testData, true: testTarget})\n","        epoch_test_acc = sess.run(final_acc, feed_dict ={x: testData, true: testTarget})\n","        print(\"Number of epoch: {}\".format(i))\n","\n","        #Now we add it to the list\n","        train_loss.append(epoch_train_loss)\n","        train_acc.append(epoch_train_acc)\n","        val_loss.append(epoch_val_loss)\n","        val_acc.append(epoch_val_acc)\n","        test_loss.append(epoch_test_loss)\n","        test_acc.append(epoch_test_acc)\n","\n","        #Now shuffle the data after each epoch\n","        trainData, trainTarget = shuffle(trainData, trainTarget)\n","\n","#First Graph for the Loss\n","n = len(train_loss)\n","plt.title(\"Train, Validation and Test Loss (dropout = 0.75)\")\n","n = len(train_loss) # number of epochs\n","plt.plot(range(1,n+1), train_loss, label=\"Train\")\n","plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n","plt.plot(range(1, n+1), test_loss, label = \"Test\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Cross Entropy Loss\")\n","plt.legend(loc='best')\n","plt.show()\n","\n","#Second Graph for the Accuracy\n","plt.title(\"Train, Validation and Test Accuracy (dropout = 0.75)\")\n","plt.plot(range(1,n+1), train_acc, label=\"Train\")\n","plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n","plt.plot(range(1, n+1), test_acc, label = \"Test\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend(loc='best')\n","plt.show()\n","\n","n = len(train_loss)\n","print(\"This is the final Training Loss: {0:.4f}\".format(train_loss[n - 1]))\n","print(\"This is the final Validation Loss: {0:.4f}\".format(val_loss[n - 1]))\n","print(\"This is the final Testing Loss: {0:.4f}\".format(test_loss[n - 1]))\n","print(\"This is the final Training Accuracy: {0:.4f}%\".format(train_acc[n - 1] * 100))\n","print(\"This is the final Validation Accuracy: {0:.4f}%\".format(val_acc[n - 1] * 100))\n","print(\"This is the final Testing Accuracy: {0:.4f}%\".format(test_acc[n - 1] * 100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EAhveNxiPJPy","colab_type":"text"},"source":["#**THIS IS THE CODE FOR PART ONE**"]},{"cell_type":"code","metadata":{"id":"tWcMUROMbQ1Q","colab_type":"code","colab":{}},"source":["def computeLayer(X, W, b):\n","    '''\n","    x (28*28, B)\n","    w_h(28*28, N)\n","    b_h (1, N)\n","    #(N, 28*28) * (28*28, B) + (N,1) -> (N, B)\n","\n","    z (N, B)\n","    w_o(N, 10)\n","    b_o (1, 10)\n","    (10, N) * (N, B) + (10,1) -> (10, B)\n","    '''\n","    return ((X@W) +b)\n","\n","def softmax(x):\n","  expos = np.exp(x - np.max(x))\n","  return expos / np.sum(expos, axis=1).reshape(-1, 1)\n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","def gradCE(target, prediction):\n","    return softmax(prediction) - target\n","\n","def gradRelu(x):\n","    x[x<0] = 0\n","    x[x>0] = 1\n","    return x\n","\n","def CE(target, prediction):\n","  return -1*(target*np.log(prediction)).mean()\n","\n","def accuracy(prediction, target): \n","  #(b, 10) (b, 10)\n","    # print(prediction)\n","    max_indices_pred = prediction.argmax(axis=1)\n","    indices_target = target.argmax(axis=1)\n","    # print(max_indices_pred, indices_target)\n","    accurate_arr = np.equal(max_indices_pred, indices_target)\n","    return np.sum((accurate_arr==True))/(indices_target.shape[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3DP53eDYdfHd","colab_type":"code","colab":{}},"source":["def forward_propagation(x, W_h, b_h, W_o, b_o):\n","    s_h = computeLayer(x, W_h, b_h)\n","    x_h = relu(s_h)\n","    s_o = computeLayer(x_h, W_o, b_o)\n","    x_o = softmax(s_o)\n","    return (s_h, x_h, s_o, x_o)\n","\n","def back_propagation(trainTarget, trainData, s_h, s_o, x_h, W_o):\n","    change_out = gradCE(trainTarget, s_o)\n","    grad_relu = gradRelu(s_h)\n","    step = np.matmul(change_out,W_o.transpose())\n","    change_hidden = grad_relu * step\n","\n","    grad_W_h = np.matmul(trainData.transpose(), change_hidden)\n","    grad_W_o = np.matmul(x_h.transpose(), change_out)\n","    grad_b_h = np.sum(change_hidden, axis = 0)\n","    grad_b_o = np.sum(change_out, axis = 0)\n","\n","    return(grad_W_h, grad_W_o, grad_b_h, grad_b_o)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvWanljIUDJB","colab_type":"code","colab":{}},"source":["def NN():\n","    np.random.seed(21)\n","    trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n","    trainTarget, validTarget, testTarget = convertOneHot(trainTarget, validTarget, testTarget)\n","\n","    #Change the shape of all the dataset\n","    trainData = trainData.reshape((trainData.shape[0], trainData.shape[1]*trainData.shape[2]))\n","    validData = validData.reshape((validData.shape[0], validData.shape[1]*validData.shape[2]))\n","    testData = testData.reshape((testData.shape[0], testData.shape[1]*testData.shape[2]))\n","\n","    #Set up the parameters\n","    hidden_units = 2000\n","    num_epochs = 200\n","    classes = 10\n","    learning_rate = 1e-5\n","    gamma = 0.99\n","\n","    #Set up the array for the loss and accuracy\n","    train_loss, val_loss, test_loss = [], [], []\n","    train_acc, val_acc, test_acc = [], [], []\n","\n","\n","    #Set up all the weights and bias following Xiaver initiazation scheme\n","    W_h = np.random.normal(0, np.sqrt(2.0/(784 + hidden_units)), (784, hidden_units))\n","    W_o = np.random.normal(0, np.sqrt(2.0/(classes + hidden_units)), (hidden_units, classes))\n","\n","    b_h = np.zeros((1, hidden_units))\n","    b_o = np.zeros((1, classes))\n","\n","    #Set up all the learning weights\n","    vW_h = np.full((784, hidden_units), 1e-5)\n","    vW_o = np.full((hidden_units, classes), 1e-5)\n","    vb_h = np.full((1, hidden_units), 1e-5)\n","    vb_o = np.full((1, classes), 1e-5)\n","\n","    for epoch in range(num_epochs):\n","          #Shuffle the data each epoch\n","          trainData, trainTarget = shuffle(trainData, trainTarget)\n","\n","          #Forward propagation\n","          s_h, x_h, s_o, x_o = forward_propagation(trainData, W_h, b_h, W_o, b_o)\n","\n","          #Back propagation\n","          grad_W_h, grad_W_o, grad_b_h, grad_b_o = back_propagation(trainTarget, trainData, s_h, s_o, x_h, W_o)\n","\n","          #Change the learning weights\n","          vW_h = (learning_rate*grad_W_h) + (gamma*vW_h)\n","          vW_o = (learning_rate*grad_W_o) + (gamma*vW_o)\n","          vb_h = (learning_rate*grad_b_h) + (gamma*vb_h)\n","          vb_o = (learning_rate*grad_b_o) + (gamma*vb_o)\n","\n","          #Add the learning weight to the actual weights\n","          W_h = W_h - vW_h\n","          W_o = W_o - vW_o\n","          b_h = b_h - vb_h\n","          b_o = b_o - vb_o\n","\n","          #Find the accuracy for the training dataset\n","          acc = accuracy(x_o, trainTarget)\n","          train_acc.append(acc*100)\n","\n","          #Find the loss for the training dataset\n","          loss = CE(trainTarget, x_o)\n","          train_loss.append(loss)\n","\n","          #Find the x_o for the validation set\n","          s_h_val, x_h_val, s_o_val, x_o_val = forward_propagation(validData, W_h, b_h, W_o, b_o)\n","\n","          #Find the accuracy for the validation dataset\n","          acc = accuracy(x_o_val, validTarget)\n","          val_acc.append(acc*100)\n","\n","          #Find the loss for the validation dataset\n","          loss = CE(validTarget, x_o_val)\n","          val_loss.append(loss)\n","\n","          #Find the x_o for the test set\n","          s_h_test, x_h_test, s_o_test, x_o_test = forward_propagation(testData, W_h, b_h, W_o, b_o)\n","\n","          #Find the accuracy for the testing dataset)\n","          acc = accuracy(x_o_test, testTarget)\n","          test_acc.append(acc*100)\n","\n","          #Find the loss for the testing dataset\n","          loss = CE(testTarget, x_o_test)\n","          test_loss.append(loss)  \n","          print(\"Epoch: {}\".format(epoch))\n","    \n","    #First Graph for the Loss\n","    n = len(train_loss)\n","    plt.title(\"Train, Validation and Test Loss \")\n","    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n","    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n","    plt.plot(range(1, n+1), test_loss, label = \"Test\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Cross Entropy Loss\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    #Second Graph for the Accuracy\n","    n = len(train_acc)\n","    plt.title(\"Train, Validation and Test Accuracy\")\n","    plt.plot(range(1,n+1), train_acc, label=\"Train\")\n","    plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n","    plt.plot(range(1, n+1), test_acc, label = \"Test\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    n = len(train_loss)\n","    print(\"This is the final Training Loss: {0:.4f}\".format(train_loss[n - 1]))\n","    print(\"This is the final Validation Loss: {0:.4f}\".format(val_loss[n - 1]))\n","    print(\"This is the final Testing Loss: {0:.4f}\".format(test_loss[n - 1]))\n","    print(\"This is the final Training Accuracy: {0:.4f}%\".format(train_acc[n - 1]))\n","    print(\"This is the final Validation Accuracy: {0:.4f}%\".format(val_acc[n - 1]))\n","    print(\"This is the final Testing Accuracy: {0:.4f}%\".format(test_acc[n - 1]))\n","\n","    return(n)\n","\n","x = NN()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_IWAbGfwaVV9","colab_type":"text"},"source":["#**PART ONE VERSION 2**"]},{"cell_type":"code","metadata":{"id":"IQEosO1LaZRy","colab_type":"code","colab":{}},"source":["# def train(train_data, valid_data, test_data, train_target, valid_target, test_target, epochs, alpha, gamma, neurons):\n","#     '''\n","#     train data (B, 28*28)\n","#     train_target (B, 10)\n","#     '''\n","\n","#     stddev_h = np.sqrt(2/(train_data.shape[0] + neurons))\n","#     w_h = np.random.normal(0, stddev_h, (train_data.shape[1], neurons)) #(28*28, N)\n","#     v_h = np.full((train_data.shape[1], neurons), 10**-5) #(28*28, N)\n","#     b_h = np.zeros((1, neurons)) #(1, N)\n","\n","#     stddev_o = np.sqrt(2/(neurons+10))\n","#     w_o = np.random.normal(0, stddev_o, (neurons,10)) #(N, 10)\n","#     v_o = np.full((neurons, 10), 10**-5)  #(N, 10)\n","#     b_o = np.zeros((1, 10)) #(1, 10)\n","   \n","\n","#     train_loss, val_loss, test_loss, train_acc, val_acc, test_acc = [], [], [], [], [], []\n","\n","#     for epoch in range(epochs):\n","#         print('Epoch: ' + str(epoch))\n","\n","#         #test test\n","#         y, s2, s1, z = for_prop(test_data, w_o, w_h, b_o, b_h)\n","#         loss = CE(test_target, y)\n","#         test_loss.append(loss)\n","#         acc = accuracy(y, test_target)\n","#         test_acc.append(acc)\n","        \n","#         #test valid\n","#         y, s2, s1, z = for_prop(valid_data, w_o, w_h, b_o, b_h)\n","#         loss = CE(valid_target, y)\n","#         val_loss.append(loss)\n","#         acc = accuracy(y, valid_target)\n","#         val_acc.append(acc)\n","\n","#         #test train\n","#         y, s2, s1, z = for_prop(train_data, w_o, w_h, b_o, b_h)\n","#         loss = CE(train_target, y)\n","#         train_loss.append(loss)\n","#         acc = accuracy(y, train_target)\n","#         train_acc.append(acc)\n","        \n","#         #back prop\n","#         dldw_o, dldb_o, dldw_h, dldb_h = back_prop(train_target, train_data, w_o, z, s1, y)\n","        \n","#         #Update the gradients \n","#         b_h -= alpha * dldb_h\n","#         b_o -= alpha * dldb_o\n","\n","#         v_h = gamma * v_h + alpha * dldw_h\n","#         v_o = gamma * v_o + alpha * dldw_o\n","\n","#         w_h -= v_h \n","#         w_o -= v_o \n","\n","      \n","\n","#     plot(epochs, train_loss, val_loss, test_loss, train_acc, val_acc, test_acc)\n","    \n","# def plot(epochs, train_loss, val_loss, test_loss, train_acc, val_acc, test_acc):\n","#     plt.figure(figsize=(20,10))\n","#     plt.title(\"Training Curve Loss\")\n","#     plt.plot(range(epochs), train_loss, label=\"Train loss\")\n","#     plt.plot(range(epochs), val_loss, label=\"Valid loss\")\n","#     plt.plot(range(epochs), test_loss, label=\"Test loss\")\n","#     plt.xlabel(\"Epochs\")\n","#     plt.ylabel(\"Loss\")\n","#     plt.legend()\n","#     plt.show()\n","\n","#     plt.figure(figsize=(20,10))\n","#     plt.title(\"Training Curve Accuracy\")\n","#     plt.plot(range(epochs), train_acc, label=\"Train accuracy\")\n","#     plt.plot(range(epochs), val_acc, label=\"Validation accuracy\")\n","#     plt.plot(range(epochs), test_acc, label=\"Test accuracy\")\n","#     plt.xlabel(\"Epochs\")\n","#     plt.ylabel(\"Training Accuracy\")\n","#     plt.legend()\n","#     plt.show()\n","    \n","#     print('Most Recent Training Accuracy = ' + str(train_acc[epochs-1]))\n","#     print('Most Recent Validation Accuracy = ' + str(val_acc[epochs-1]))\n","#     print('Most Recent Test Accuracy = ' + str(test_acc[epochs-1]))\n","#     print('Max Training Accuracy = ' + str(max(train_acc)))\n","#     print('Max Validation Accuracy = ' + str(max(val_acc)))\n","#     print('Max Test Accuracy = ' + str(max(test_acc)))\n","#     print('Most Recent Training Loss = ' + str(train_loss[epochs-1]))\n","#     print('Most Recent Validation Loss = ' + str(val_loss[epochs-1]))\n","#     print('Most Recent Test Loss = ' + str(test_loss[epochs-1]))\n","#     print('Min Training Loss = ' + str(min(train_loss)))\n","#     print('Min Validation Loss = ' + str(min(val_loss)))\n","#     print('Min Test Loss = ' + str(min(test_loss)))"],"execution_count":0,"outputs":[]}]}