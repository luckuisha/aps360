{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet Transfer Learning.ipynb","provenance":[{"file_id":"1gUAAWGdh7RzOjM9UBq93XzLz4iY3Jbmu","timestamp":1585590369502}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"76e6b87ea6ab4281ac107ec733e51ce9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b63df3ebb16146b39d3a6e783fda30af","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f0f14fbf8ab84644a3233ad7c20dd2f5","IPY_MODEL_61793385bc374fdbbf52a52370b3a3ae"]}},"b63df3ebb16146b39d3a6e783fda30af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f0f14fbf8ab84644a3233ad7c20dd2f5":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4b62bb6940eb4315a264dbdcd88f5490","_dom_classes":[],"description":"100%","_model_name":"IntProgressModel","bar_style":"success","max":241530880,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":241530880,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a5817cbae101493eb148519a55fef52f"}},"61793385bc374fdbbf52a52370b3a3ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b0a99d6b46bd47e9aa071da9768e6370","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 230M/230M [00:05&lt;00:00, 47.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_13dba0379f664b90a4a9c63a0cc3bc98"}},"4b62bb6940eb4315a264dbdcd88f5490":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a5817cbae101493eb148519a55fef52f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b0a99d6b46bd47e9aa071da9768e6370":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"13dba0379f664b90a4a9c63a0cc3bc98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"ujyxkPoGNuaC","colab_type":"code","colab":{}},"source":["# Transfer learning using GoogLeNet"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ewqW2uAN_bG","colab_type":"code","outputId":"e5e1682a-cb41-40d4-b265-f7e87e9f1497","executionInfo":{"status":"ok","timestamp":1585707140214,"user_tz":240,"elapsed":19426,"user":{"displayName":"lucky kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0nu-vSwn0mi80R-2RTsD0Y5a0-k6BhwFB4gRf7w=s64","userId":"07630087795763678594"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["'''\n","importing necessary libraries\n","'''\n","import math\n","from shutil import copyfile\n","import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt # for plotting\n","import os\n","import sys\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zdSBq_BIOEYH","colab_type":"code","outputId":"1b665322-ec66-481c-b0c1-281bbcb63897","executionInfo":{"status":"ok","timestamp":1585707190138,"user_tz":240,"elapsed":7980,"user":{"displayName":"lucky kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0nu-vSwn0mi80R-2RTsD0Y5a0-k6BhwFB4gRf7w=s64","userId":"07630087795763678594"}},"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["76e6b87ea6ab4281ac107ec733e51ce9","b63df3ebb16146b39d3a6e783fda30af","f0f14fbf8ab84644a3233ad7c20dd2f5","61793385bc374fdbbf52a52370b3a3ae","4b62bb6940eb4315a264dbdcd88f5490","a5817cbae101493eb148519a55fef52f","b0a99d6b46bd47e9aa071da9768e6370","13dba0379f664b90a4a9c63a0cc3bc98"]}},"source":["# for importing GoogLeNet pretrained model\n","from torchvision import models\n","resnet152 = models.resnet152(pretrained=True, progress=True)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76e6b87ea6ab4281ac107ec733e51ce9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=241530880), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dp6NadMbYZOh","colab_type":"text"},"source":["##Regular Data Loader"]},{"cell_type":"code","metadata":{"id":"YzdaF1SKOEl_","colab_type":"code","colab":{}},"source":["# get dataloaders using the 1000-image dataset\n","def get_data_loader(batch_size=32):\n","\n","    np.random.seed(1000) # set the seed for reproducible shuffling\n","    num_workers = 1\n","\n","    # define the training, validation, and testing directories to the smaller dataset\n","    train_path = '/content/drive/My Drive/3rd year/2nd semester/aps360/APS360 Project/1000_set/train/'\n","    valid_path = '/content/drive/My Drive/3rd year/2nd semester/aps360/APS360 Project/1000_set/valid/'\n","    test_path = '/content/drive/My Drive/3rd year/2nd semester/aps360/APS360 Project/1000_set/test/'\n","\n","    # convert all jpgs to tensors\n","    data_transform = transforms.Compose([transforms.Resize((224,224)), \n","                                    transforms.ToTensor()])\n","\n","    # load training, validation, and testing data\n","    train_data = torchvision.datasets.ImageFolder(root = train_path, \n","                                            transform=data_transform)\n","\n","    val_data = torchvision.datasets.ImageFolder(root = valid_path, \n","                                            transform=data_transform)\n","\n","    test_data = torchvision.datasets.ImageFolder(root = test_path, \n","                                            transform=data_transform)\n","    \n","    # get dataset loaders\n","    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n","                                            num_workers=num_workers, shuffle=True)\n","\n","    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, \n","                                            num_workers=num_workers, shuffle=True)\n","\n","    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n","                                            num_workers=num_workers, shuffle=True)\n","\n","    return train_loader, val_loader, test_loader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v-hr3MkGYbcZ","colab_type":"text"},"source":["##Feature Loader\n","- Which I don't think I ever call but whatever"]},{"cell_type":"code","metadata":{"id":"3-v_nNa6Wk_Y","colab_type":"code","colab":{}},"source":["# get the feature loaders\n","def get_feature_loaders(batch_size):\n","    trainLoader, valLoader, testLoader = get_data_loader(batch_size)\n","\n","    trainFeatures, valFeatures, testFeatures = []\n","    trainLabels, valLabels, testLabels = []\n","\n","    for i, data in enumerate(trainLoader, 1):\n","        # Get the inputs\n","        inputs, labels = data\n","        trainFeatures.append(inputs)\n","        trainLabels.append(labels)\n","\n","    for i, data in enumerate(valLoader, 1):\n","        # Get the inputs\n","        inputs, labels = data\n","        valFeatures.append(inputs)\n","        valLabels.append(labels)\n","\n","    for i, data in enumerate(testLoader, 1):\n","        # Get the inputs\n","        inputs, labels = data\n","        testFeatures.append(inputs)\n","        testLabels.append(labels)\n","\n","    return trainFeatures, valFeatures, testFeatures, trainLabels, valLabels, testLabels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VivKZv29YPyZ","colab_type":"text"},"source":["##Save Features to a Folder\n","- Prevents us from recomputing the features every time the thing is run"]},{"cell_type":"code","metadata":{"id":"IHyePUE5XKhb","colab_type":"code","outputId":"6c78a31e-53b5-4789-f0f7-ff2bc70f51dc","executionInfo":{"status":"error","timestamp":1585590514763,"user_tz":240,"elapsed":19664,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-7ot31psTGtAXzKlWBL6OEtJ7v4FOjLQdRdeG=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":346}},"source":["# Save Features to Folder (assumes code from 1. has been evaluated)\n","import os\n","\n","# location on Google Drive\n","master_path = '/content/drive/My Drive/3rd year/2nd semester/aps360/APS360 Project/1000_set/test/'\n","\n","train_loader, val_loader, test_loader = get_data_loader(1)\n","\n","# the food categories in the dataset\n","classes = ['Bread', 'Dairy product', 'Vegetable/Fruit', 'Dessert', 'Egg', 'Fried food', 'Meat', 'Noodles/Pasta', 'Rice', 'Seafood', 'Soup']\n","\n","# save features to folder as tensors\n","i = 0\n","for img, label in train_loader:\n","  features = googlenet.features(img)\n","  features_tensor = torch.from_numpy(features.detach().numpy())\n","\n","  folder_name = master_path + '/train/' +  str(classes[label])\n","  if not os.path.isdir(folder_name):\n","    os.mkdir(folder_name)\n","  torch.save(features_tensor.squeeze(0), folder_name + '/' + str(i) + '.tensor')\n","  i += 1\n","\n","j = 0\n","for img, label in val_loader:\n","  features = googlenet.features(img)\n","  features_tensor = torch.from_numpy(features.detach().numpy())\n","\n","  folder_name = master_path + '/val/' +  str(classes[label])\n","  if not os.path.isdir(folder_name):\n","    os.mkdir(folder_name)\n","  torch.save(features_tensor.squeeze(0), folder_name + '/' + str(j) + '.tensor')\n","  j += 1\n","\n","k = 0\n","for img, label in test_loader:\n","  features = googlenet.features(img)\n","  features_tensor = torch.from_numpy(features.detach().numpy())\n","\n","  folder_name = master_path + '/test/' +  str(classes[label])\n","  if not os.path.isdir(folder_name):\n","    os.mkdir(folder_name)\n","  torch.save(features_tensor.squeeze(0), folder_name + '/' + str(k) + '.tensor')\n","  k += 1"],"execution_count":0,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-76daa2e8a7dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgooglenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mfeatures_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 576\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'GoogLeNet' object has no attribute 'features'"]}]},{"cell_type":"markdown","metadata":{"id":"ayZrPZ4dYMNV","colab_type":"text"},"source":["##GoogLeNet Data Loader"]},{"cell_type":"code","metadata":{"id":"dl2BjizJXuol","colab_type":"code","colab":{}},"source":["def googlenet_data_loader(batch_size):\n","    np.random.seed(1000) # set the seed for reproducible shuffling\n","\n","    master_path = '/content/drive/My Drive/Colab Notebooks/APS360/APS360 Project/GoogLeNet'\n","    googlenet_train_path = master_path + '/train'\n","    googlenet_val_path = master_path + '/val'\n","    googlenet_test_path = master_path + '/test'\n","\n","    googlenet_train_dataset = torchvision.datasets.DatasetFolder(googlenet_train_path, loader=torch.load, extensions=('.tensor'))\n","    googlenet_val_dataset = torchvision.datasets.DatasetFolder(googlenet_val_path, loader=torch.load, extensions=('.tensor'))\n","    googlenet_test_dataset = torchvision.datasets.DatasetFolder(googlenet_test_path, loader=torch.load, extensions=('.tensor'))\n","\n","    # Prepare Dataloader\n","    num_workers = 1\n","    googlenet_train_loader = torch.utils.data.DataLoader(googlenet_train_dataset, batch_size=batch_size, \n","                                            num_workers=num_workers, shuffle=True)\n","    googlenet_val_loader = torch.utils.data.DataLoader(googlenet_val_dataset, batch_size=batch_size, \n","                                            num_workers=num_workers, shuffle=True)\n","    googlenet_test_loader = torch.utils.data.DataLoader(googlenet_test_dataset, batch_size=batch_size, \n","                                            num_workers=num_workers, shuffle=True)\n","    return googlenet_train_loader, googlenet_val_loader, googlenet_test_loader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5yId53YVYolX","colab_type":"text"},"source":["##Verification Step\n","- To determine the size of the input images"]},{"cell_type":"code","metadata":{"id":"YfUvWpY9YoZ3","colab_type":"code","colab":{}},"source":["# Verification Step - obtain one batch of features\n","\n","sample_stuff, _, _ = googlenet_data_loader(32)\n","\n","dataiter = iter(sample_stuff)\n","features, labels = dataiter.next()\n","print(\"features dimensions:\", features.shape)\n","print(\"labels dimensions:\", labels.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wDIg-EJcYHuU","colab_type":"text"},"source":["##Neural Network Architecture\n","- Just an ANN for now, add CNN later maybe???"]},{"cell_type":"code","metadata":{"id":"N2RuHDKwY0GT","colab_type":"code","colab":{}},"source":["#Artifical Neural Network Architecture\n","# ----------------------------------CALCULATIONS----------------------------------\n","# there are 256 6x6 input images and 9 expected outputs\n","class residual(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(residual, self).__init__()\n","        self.name = \"residual\"\n","\n","        self.hidden_size = hidden_size\n","\n","        self.fc1 = nn.Linear(256 * 3 * 3, hidden_size)\n","        #self.fc3 = nn.Linear(2048, 512)\n","        #self.fc4 = nn.Linear(512, 32)\n","        self.fc2 = nn.Linear(hidden_size, 11)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 256 * 3 * 3) #flatten feature data\n","        x = F.relu(self.fc1(x))\n","        #x = F.relu(self.fc3(x))\n","        #x = F.relu(self.fc4(x))\n","        x = self.fc2(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xoq7y5mZtLp","colab_type":"text"},"source":["##Calculate Accuracy"]},{"cell_type":"code","metadata":{"id":"A2UQPm3LZwGA","colab_type":"code","colab":{}},"source":["def get_accuracy_resnet(model, loader, loss_function):\n","    correct = 0\n","    loss2 = 0\n","    num_evaluated = 0\n","\n","    for num_batches, data in enumerate(loader, 1):\n","        imgs, labels = data\n","\n","        if torch.cuda.is_available():\n","          imgs = imgs.cuda()\n","          labels = labels.cuda()\n","\n","        # determine accuracy\n","        prediction = model(imgs)\n","        pred = prediction.max(1, keepdim=True)[1] #select index with maximum prediction score\n","        correct += pred.eq(labels.view_as(pred)).sum().item()\n","\n","        # determine loss\n","        loss1 = loss_function(prediction, labels.long())\n","        loss2 += loss1.item()\n","\n","        num_evaluated += len(labels) # this is how many labels you just evaluated\n","\n","    # accuracy: total accuracy / number of items evaluated\n","    accuracy_rate = float(correct) / num_evaluated \n","    # loss: total loss / batch size evaluated\n","    loss_rate = float(loss2) / num_batches\n","    \n","    return accuracy_rate, loss_rate"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tVYPhQmNabJE","colab_type":"text"},"source":["##Plot Graphs"]},{"cell_type":"code","metadata":{"id":"dcTzjTdtacis","colab_type":"code","colab":{}},"source":["  def plot_graph(graph_title, x_label, y_label, num_epochs, training_data, val_data, testing_data = None):\n","    plt.figure()\n","    plt.title(graph_title)\n","    plt.xlabel(x_label)\n","    plt.ylabel(y_label)\n","    \n","    plt.plot(range(1,num_epochs+1), training_data, label=\"Training\")\n","    plt.plot(range(1,num_epochs+1), val_data, label=\"Validation\")\n","\n","    if testing_data != None:\n","        plt.plot(range(1,num_epochs+1), testing_data, label=\"Testing\")\n","    plt.legend()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y9r6RB5wZ3Qx","colab_type":"text"},"source":["##Training Function"]},{"cell_type":"code","metadata":{"id":"91h3-AS9Z46r","colab_type":"code","colab":{}},"source":["def train_resnet(model, batch_size=64, learning_rate = 0.01, num_epochs=30):\n","    np.random.seed(1000) # set the seed for reproducible shuffling\n","\n","    # load the correct data\n","    train_loader, val_loader, test_loader = googlenet_data_loader(batch_size)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    print(\"Loss function used: CrossEntropyLoss\")\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    print(\"Optimizer used: Adam\")\n","\n","    # training\n","    n = 0 # the number of iterations\n","    train_err = np.zeros(num_epochs)\n","    train_loss = np.zeros(num_epochs)\n","    val_err = np.zeros(num_epochs)\n","    val_loss = np.zeros(num_epochs)\n","    ########################################################################\n","    # Train the network\n","    # Loop over the data iterator and sample a new batch of training data\n","    # Get the output from the network, and optimize our loss function.\n","    start_time = time.time()\n","    for epoch in range(num_epochs):  # loop over the dataset multiple times\n","        for data in train_loader:\n","            # Get the inputs\n","            inputs, labels = data\n","            \n","            #############################################\n","            #To Enable GPU Usage\n","            if torch.cuda.is_available():\n","                inputs = inputs.cuda()\n","                labels = labels.cuda()\n","            #############################################\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","            \n","            # Forward pass, backward pass, and optimize\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels.long())\n","            loss.backward()\n","            optimizer.step()\n","\n","        train_err[epoch], train_loss[epoch] = get_accuracy_googlenet(model, train_loader, criterion) # accuracy function provided\n","        val_err[epoch], val_loss[epoch] = get_accuracy_googlenet(model, val_loader, criterion)\n","\n","        # Save the current model (checkpoint) to a file\n","        model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n","        torch.save(model.state_dict(), model_path)\n","\n","    print('\\nFinished Training')\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n","\n","    print(\"\\n-------------------------------------------------------------------------\")\n","    print(\"Training accuracy after {} epochs: {}\".format(num_epochs, train_err[-1]))\n","    print(\"Training loss after {} epochs: {}\".format(num_epochs, train_loss[-1]))\n","    print(\"\\n-------------------------------------------------------------------------\")\n","    print(\"Validation accuracy after {} epochs: {}\".format(num_epochs, val_err[-1]))\n","    print(\"Validation loss after {} epochs: {}\".format(num_epochs, val_loss[-1]))\n","\n","    print(\"\\n------------------------------GRAPHS------------------------------------\")\n","    print(\"\\nAccuracy plot of FoodGoogLeNet NN\")   \n","    plot_graph(\"Accuracy\", \"Number of Epochs\", \"Accuracy\", num_epochs, train_err, val_err)\n","\n","    print(\"\\nLoss plot of FoodGoogLeNet using CrossEntropyLoss\")\n","    plot_graph(\"Loss\", \"Number of Epochs\", \"Loss\", num_epochs, train_loss, val_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L50CS7DoauDh","colab_type":"text"},"source":["##Training and Hyperparameter Search"]},{"cell_type":"code","metadata":{"id":"3QmWIxm0avSn","colab_type":"code","colab":{}},"source":["# batch size: 64\n","# learning rate: 0.0001\n","# number of epochs: 30\n","# number of layers: 1 fully-connected layer\n","\n","model = residual(2048)\n","\n","if torch.cuda.is_available():\n","    print(\"Using GPU...\")\n","    model = model.cuda()\n","else:\n","    print(\"Using CPU...\")\n","\n","train_googlenet(model, batch_size = 64, learning_rate = 0.0001, num_epochs=30)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v6lz9oiPa6uV","colab_type":"text"},"source":["##Running on Training Dataset"]},{"cell_type":"code","metadata":{"id":"bFatsQAIa_0a","colab_type":"code","colab":{}},"source":["_, _, test_loader = googlenet_data_loader(batch_size = 64)\n","\n","model = FoodGoogLeNet()\n","\n","if torch.cuda.is_available():\n","    print(\"Using GPU...\")\n","    model = model.cuda()\n","else:\n","    print(\"Using CPU...\")\n","\n","model_path = get_model_name(model.name, batch_size=64, learning_rate=0.0001, epoch=29)\n","state = torch.load(model_path)\n","model.load_state_dict(state)\n","\n","accuracy, loss = get_accuracy_googlenet(model, test_loader, nn.CrossEntropyLoss())\n","\n","print(\"The testing acuuracy is:\", accuracy)\n","print(\"The testing loss is:\", loss)"],"execution_count":0,"outputs":[]}]}