{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGGTransferLearning.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1gUAAWGdh7RzOjM9UBq93XzLz4iY3Jbmu","authorship_tag":"ABX9TyNLg3tPycXkOeqh9PuhAMw0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"479c5e825555439c8880a9483ea94bbf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d0134951d5f045878759cf29db739f6d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f5948174b01f490a8befc731707bbc88","IPY_MODEL_894c42b49c5b4c8face976d2869d9e31"]}},"d0134951d5f045878759cf29db739f6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f5948174b01f490a8befc731707bbc88":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f486a2bbca71464aad23380cddf2d6b5","_dom_classes":[],"description":"100%","_model_name":"IntProgressModel","bar_style":"success","max":553433881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":553433881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_95ef81ab4cdd4e8fbca9dc662af64bbd"}},"894c42b49c5b4c8face976d2869d9e31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e0dccd3317844703860e21553a566519","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 528M/528M [00:32&lt;00:00, 16.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d42988ed0f9b4252b58baf0fb7254265"}},"f486a2bbca71464aad23380cddf2d6b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"95ef81ab4cdd4e8fbca9dc662af64bbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0dccd3317844703860e21553a566519":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d42988ed0f9b4252b58baf0fb7254265":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"ujyxkPoGNuaC","colab_type":"code","colab":{}},"source":["# Transfer learning using VGG"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ewqW2uAN_bG","colab_type":"code","colab":{}},"source":["'''\n","importing necessary libraries\n","'''\n","import math\n","from shutil import copyfile\n","import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt # for plotting\n","import os\n","import sys"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SUdHU1sV3Li","colab_type":"code","outputId":"3641bae1-98ad-4e37-d9b9-8f61b3a63e5c","executionInfo":{"status":"ok","timestamp":1585771693928,"user_tz":240,"elapsed":2645,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-7ot31psTGtAXzKlWBL6OEtJ7v4FOjLQdRdeG=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zdSBq_BIOEYH","colab_type":"code","outputId":"286fcf05-26ce-4689-e18f-b2c9aa421d52","executionInfo":{"status":"ok","timestamp":1585771708047,"user_tz":240,"elapsed":15746,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-7ot31psTGtAXzKlWBL6OEtJ7v4FOjLQdRdeG=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":82,"referenced_widgets":["479c5e825555439c8880a9483ea94bbf","d0134951d5f045878759cf29db739f6d","f5948174b01f490a8befc731707bbc88","894c42b49c5b4c8face976d2869d9e31","f486a2bbca71464aad23380cddf2d6b5","95ef81ab4cdd4e8fbca9dc662af64bbd","e0dccd3317844703860e21553a566519","d42988ed0f9b4252b58baf0fb7254265"]}},"source":["# for importing VGG pretrained model\n","import torchvision.models\n","vgg = torchvision.models.vgg16(pretrained=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"479c5e825555439c8880a9483ea94bbf","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=553433881), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dp6NadMbYZOh","colab_type":"text"},"source":["##Regular Data Loader"]},{"cell_type":"code","metadata":{"id":"YzdaF1SKOEl_","colab_type":"code","colab":{}},"source":["# get dataloaders using the 1000-image dataset\n","def get_data_loader(batch_size=32):\n","\n","    np.random.seed(1000) # set the seed for reproducible shuffling\n","    num_workers = 1\n","\n","    # define the training, validation, and testing directories to the smaller dataset\n","    train_path = '/content/drive/My Drive/Colab Notebooks/APS360/APS360 Project/food-11/training'\n","    valid_path = '/content/drive/My Drive/Colab Notebooks/APS360/APS360 Project/food-11/validation'\n","    test_path = '/content/drive/My Drive/Colab Notebooks/APS360/APS360 Project/food-11/evaluation'\n","\n","    # convert all jpgs to tensors\n","    data_transform = transforms.Compose([transforms.Resize((224,224)), \n","                                    transforms.ToTensor()])\n","\n","    # load training, validation, and testing data\n","    train_data = torchvision.datasets.ImageFolder(root = train_path, \n","                                            transform=data_transform)\n","\n","    val_data = torchvision.datasets.ImageFolder(root = valid_path, \n","                                            transform=data_transform)\n","\n","    test_data = torchvision.datasets.ImageFolder(root = test_path, \n","                                            transform=data_transform)\n","    \n","    # get dataset loaders\n","    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n","                                            num_workers=num_workers, shuffle=True)\n","\n","    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, \n","                                            num_workers=num_workers, shuffle=True)\n","\n","    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n","                                            num_workers=num_workers, shuffle=True)\n","\n","    return train_loader, val_loader, test_loader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v-hr3MkGYbcZ","colab_type":"text"},"source":["##Feature Loader\n","- Which I don't think I ever call but whatever"]},{"cell_type":"code","metadata":{"id":"3-v_nNa6Wk_Y","colab_type":"code","colab":{}},"source":["# get the feature loaders\n","def get_feature_loaders(batch_size):\n","    trainLoader, valLoader, testLoader = get_data_loader(batch_size)\n","\n","    trainFeatures, valFeatures, testFeatures = []\n","    trainLabels, valLabels, testLabels = []\n","\n","    for i, data in enumerate(trainLoader, 1):\n","        # Get the inputs\n","        inputs, labels = data\n","        trainFeatures.append(inputs)\n","        trainLabels.append(labels)\n","\n","    for i, data in enumerate(valLoader, 1):\n","        # Get the inputs\n","        inputs, labels = data\n","        valFeatures.append(inputs)\n","        valLabels.append(labels)\n","\n","    for i, data in enumerate(testLoader, 1):\n","        # Get the inputs\n","        inputs, labels = data\n","        testFeatures.append(inputs)\n","        testLabels.append(labels)\n","\n","    return trainFeatures, valFeatures, testFeatures, trainLabels, valLabels, testLabels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VivKZv29YPyZ","colab_type":"text"},"source":["##Save Features to a Folder\n","- Prevents us from recomputing the features every time the thing is run\n","- ALREADY RAN IT ONCE DO NOT RUN THIS AGAIN UNLESS YOU'RE PREPARED TO WAIT 3 HOURS"]},{"cell_type":"code","metadata":{"id":"IHyePUE5XKhb","colab_type":"code","colab":{}},"source":["# Save Features to Folder (assumes code from 1. has been evaluated)\n","\n","# location on Google Drive\n","master_path = os.getcwd() + \"/drive/My Drive/Colab Notebooks/APS360/APS360 Project/VGG\"\n","#print(os.getcwd())\n","#os.chdir('/content/drive/My Drive/Colab Notebooks/APS360/APS360 Project/VGG')\n","#!ls\n","\n","train_loader, val_loader, test_loader = get_data_loader(1)\n","\n","# the food categories in the dataset\n","classes = ['Bread', 'Dairy', 'VegetableFruit', 'Dessert', 'Egg', 'FriedFood', 'Meat', 'NoodlesPasta', 'Rice', 'Seafood', 'Soup']\n","\n","# save features to folder as tensors\n","i = 0\n","for img, label in train_loader:\n","  features = vgg.features(img)\n","  features_tensor = torch.from_numpy(features.detach().numpy())\n","  \n","  #print(str(classes[label]))\n","\n","  folder_name = master_path + '/train/' +  str(classes[label])\n","  if not os.path.isdir(folder_name):\n","    os.mkdir(folder_name)\n","  torch.save(features_tensor.squeeze(0), folder_name + '/' + str(i) + '.tensor')\n","  i += 1\n","\n","j = 0\n","for img, label in val_loader:\n","  features = vgg.features(img)\n","  features_tensor = torch.from_numpy(features.detach().numpy())\n","\n","  folder_name = master_path + '/val/' +  str(classes[label])\n","  if not os.path.isdir(folder_name):\n","    os.mkdir(folder_name)\n","  torch.save(features_tensor.squeeze(0), folder_name + '/' + str(j) + '.tensor')\n","  j += 1\n","\n","k = 0\n","for img, label in test_loader:\n","  features = vgg.features(img)\n","  features_tensor = torch.from_numpy(features.detach().numpy())\n","\n","  folder_name = master_path + '/test/' +  str(classes[label])\n","  if not os.path.isdir(folder_name):\n","    os.mkdir(folder_name)\n","  torch.save(features_tensor.squeeze(0), folder_name + '/' + str(k) + '.tensor')\n","  k += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ayZrPZ4dYMNV","colab_type":"text"},"source":["##VGG Data Loader"]},{"cell_type":"code","metadata":{"id":"dl2BjizJXuol","colab_type":"code","colab":{}},"source":["def vgg_data_loader(batch_size):\n","    np.random.seed(1000) # set the seed for reproducible shuffling\n","\n","    master_path = '/content/drive/My Drive/Colab Notebooks/APS360/APS360 Project/VGG'\n","    vgg_train_path = master_path + '/train'\n","    vgg_val_path = master_path + '/val'\n","    vgg_test_path = master_path + '/test'\n","\n","    vgg_train_dataset = torchvision.datasets.DatasetFolder(vgg_train_path, loader=torch.load, extensions=('.tensor'))\n","    vgg_val_dataset = torchvision.datasets.DatasetFolder(vgg_val_path, loader=torch.load, extensions=('.tensor'))\n","    vgg_test_dataset = torchvision.datasets.DatasetFolder(vgg_test_path, loader=torch.load, extensions=('.tensor'))\n","\n","    # Prepare Dataloader\n","    num_workers = 1\n","    vgg_train_loader = torch.utils.data.DataLoader(vgg_train_dataset, batch_size=batch_size, \n","                                            num_workers=num_workers, shuffle=True)\n","    vgg_val_loader = torch.utils.data.DataLoader(vgg_val_dataset, batch_size=batch_size, \n","                                            num_workers=num_workers, shuffle=True)\n","    vgg_test_loader = torch.utils.data.DataLoader(vgg_test_dataset, batch_size=batch_size, \n","                                            num_workers=num_workers, shuffle=True)\n","    return vgg_train_loader, vgg_val_loader, vgg_test_loader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5yId53YVYolX","colab_type":"text"},"source":["##Verification Step\n","- To determine the size of the input images"]},{"cell_type":"code","metadata":{"id":"YfUvWpY9YoZ3","colab_type":"code","outputId":"8bd172ba-66cd-42b4-98db-1fcb5fdc0adb","executionInfo":{"status":"ok","timestamp":1585777001887,"user_tz":240,"elapsed":18141,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-7ot31psTGtAXzKlWBL6OEtJ7v4FOjLQdRdeG=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":218}},"source":["# Verification Step - obtain one batch of features\n","\n","sample_stuff, more_sample_stuff, other_sample_stuff = vgg_data_loader(32)\n","\n","dataiter = iter(sample_stuff)\n","features, labels = dataiter.next()\n","print(\"features dimensions:\", features.shape)\n","print(\"labels dimensions:\", labels.shape)\n","print(labels)\n","\n","dataiter = iter(more_sample_stuff)\n","features, labels = dataiter.next()\n","print(\"features dimensions:\", features.shape)\n","print(\"labels dimensions:\", labels.shape)\n","print(labels)\n","\n","dataiter = iter(other_sample_stuff)\n","features, labels = dataiter.next()\n","print(\"features dimensions:\", features.shape)\n","print(\"labels dimensions:\", labels.shape)\n","print(labels)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["features dimensions: torch.Size([32, 512, 7, 7])\n","labels dimensions: torch.Size([32])\n","tensor([ 8, 10,  4, 10,  2, 10, 10,  2,  2,  3,  4,  9,  2,  1,  9,  1,  2, 10,\n","         4,  5,  2,  0, 10,  6,  2,  6,  9,  9,  9, 10,  4,  6])\n","features dimensions: torch.Size([32, 512, 7, 7])\n","labels dimensions: torch.Size([32])\n","tensor([10, 10,  4,  7,  4,  3,  6, 11,  6,  9,  4, 10,  0,  2,  0, 10,  2,  0,\n","         0, 10, 10,  8,  1,  2,  8, 11, 10,  9,  3,  4,  2,  0])\n","features dimensions: torch.Size([32, 512, 7, 7])\n","labels dimensions: torch.Size([32])\n","tensor([ 1,  5,  6,  3,  0,  2,  6,  6,  2, 10,  8,  9,  8,  8,  3,  9,  8,  6,\n","         5,  0,  4,  6,  9,  8,  6, 10, 10,  3,  1,  0,  7,  7])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wDIg-EJcYHuU","colab_type":"text"},"source":["##Neural Network Architecture\n","- Just an ANN for now, add CNN later maybe???"]},{"cell_type":"code","metadata":{"id":"N2RuHDKwY0GT","colab_type":"code","colab":{}},"source":["#Artifical Neural Network Architecture\n","# ----------------------------------CALCULATIONS----------------------------------\n","# there are 512 7x7 input images and 11 expected outputs\n","class FoodVGG(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(FoodVGG, self).__init__()\n","        self.name = \"FoodVGG\"\n","\n","        self.hidden_size = hidden_size\n","\n","        self.fc1 = nn.Linear(512 * 7 * 7, hidden_size)\n","        #self.fc3 = nn.Linear(2048, 512)\n","        #self.fc4 = nn.Linear(512, 32)\n","        self.fc2 = nn.Linear(hidden_size, 11)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 512 * 7 * 7) #flatten feature data\n","        x = F.relu(self.fc1(x))\n","        #x = F.relu(self.fc3(x))\n","        #x = F.relu(self.fc4(x))\n","        x = self.fc2(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xoq7y5mZtLp","colab_type":"text"},"source":["##Calculate Accuracy"]},{"cell_type":"code","metadata":{"id":"A2UQPm3LZwGA","colab_type":"code","colab":{}},"source":["def get_accuracy_vgg(model, loader, loss_function):\n","    correct = 0\n","    loss2 = 0\n","    num_evaluated = 0\n","\n","    for num_batches, data in enumerate(loader, 1):\n","        imgs, labels = data\n","        #imgs = alexnet.features(imgs)\n","\n","        if torch.cuda.is_available():\n","          imgs = imgs.cuda()\n","          labels = labels.cuda()\n","\n","        # determine accuracy\n","        prediction = model(imgs)\n","        pred = prediction.max(1, keepdim=True)[1] #select index with maximum prediction score\n","        correct += pred.eq(labels.view_as(pred)).sum().item()\n","\n","        # determine loss\n","        loss1 = loss_function(prediction, labels.long())\n","        loss2 += loss1.item()\n","\n","        num_evaluated += len(labels) # this is how many labels you just evaluated\n","\n","    # accuracy: total accuracy / number of items evaluated\n","    accuracy_rate = float(correct) / num_evaluated \n","    # loss: total loss / batch size evaluated\n","    loss_rate = float(loss2) / num_batches\n","    \n","    return accuracy_rate, loss_rate"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tVYPhQmNabJE","colab_type":"text"},"source":["##Plot Graphs"]},{"cell_type":"code","metadata":{"id":"dcTzjTdtacis","colab_type":"code","colab":{}},"source":["def plot_graph(graph_title, x_label, y_label, num_epochs, training_data, val_data, testing_data = None):\n","    plt.figure()\n","    plt.title(graph_title)\n","    plt.xlabel(x_label)\n","    plt.ylabel(y_label)\n","    \n","    plt.plot(range(1,num_epochs+1), training_data, label=\"Training\")\n","    plt.plot(range(1,num_epochs+1), val_data, label=\"Validation\")\n","\n","    if testing_data != None:\n","        plt.plot(range(1,num_epochs+1), testing_data, label=\"Testing\")\n","    plt.legend()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TnT4jQwOX5in","colab_type":"text"},"source":["##Get Model Path"]},{"cell_type":"code","metadata":{"id":"HbRDzKsOX7mT","colab_type":"code","colab":{}},"source":["def get_model_name(name, batch_size, learning_rate, epoch):\n","    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name, batch_size, learning_rate, epoch)\n","    return path"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y9r6RB5wZ3Qx","colab_type":"text"},"source":["##Training Function"]},{"cell_type":"code","metadata":{"id":"91h3-AS9Z46r","colab_type":"code","colab":{}},"source":["def train_vgg(model, batch_size=64, learning_rate = 0.01, num_epochs=30):\n","    np.random.seed(1000) # set the seed for reproducible shuffling\n","\n","    # load the correct data\n","    train_loader, val_loader, test_loader = vgg_data_loader(batch_size)\n","    \n","    one_sample = iter(train_loader)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    print(\"Loss function used: CrossEntropyLoss\")\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    print(\"Optimizer used: Adam\")\n","\n","    # training\n","    n = 0 # the number of iterations\n","    train_err = np.zeros(num_epochs)\n","    train_loss = np.zeros(num_epochs)\n","    val_err = np.zeros(num_epochs)\n","    val_loss = np.zeros(num_epochs)\n","    ########################################################################\n","    # Train the network\n","    # Loop over the data iterator and sample a new batch of training data\n","    # Get the output from the network, and optimize our loss function.\n","    #count = 0\n","    start_time = time.time()\n","    #for epoch in range(num_epochs):  # loop over the dataset multiple times\n","    #for data in train_loader:\n","    #count += 1\n","\n","    #if count == 1000:\n","        #count = 0\n","        #break\n","\n","    # Get the inputs\n","    inputs, labels = one_sample.next()\n","\n","    #############################################\n","    #To Enable GPU Usage\n","    if torch.cuda.is_available():\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","    #############################################\n","\n","    # Zero the parameter gradients\n","    optimizer.zero_grad()\n","\n","    # Forward pass, backward pass, and optimize\n","    outputs = model(inputs)\n","    loss = criterion(outputs, labels.long())\n","    loss.backward()\n","    optimizer.step()\n","\n","    train_err[0], train_loss[0] = get_accuracy_vgg(model, train_loader, criterion) # accuracy function provided\n","    #val_err[0], val_loss[0] = get_accuracy_vgg(model, val_loader, criterion)\n","\n","    # Save the current model (checkpoint) to a file\n","    model_path = get_model_name(model.name, batch_size, learning_rate, 0)\n","    torch.save(model.state_dict(), model_path)\n","\n","    print('\\nFinished Training')\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n","\n","    print(\"\\n-------------------------------------------------------------------------\")\n","    print(\"Training accuracy after {} epochs: {}\".format(num_epochs, 1-train_err[-1]))\n","    print(\"Training loss after {} epochs: {}\".format(num_epochs, train_loss[-1]))\n","    print(\"\\n-------------------------------------------------------------------------\")\n","    #print(\"Validation accuracy after {} epochs: {}\".format(num_epochs, val_err[-1]))\n","    #print(\"Validation loss after {} epochs: {}\".format(num_epochs, val_loss[-1]))\n","\n","    # print(\"\\n------------------------------GRAPHS------------------------------------\")\n","    # print(\"\\nAccuracy plot of FoodVGG NN\")   \n","    # plot_graph(\"Accuracy\", \"Number of Epochs\", \"Accuracy\", num_epochs, train_err, val_err)\n","\n","    # print(\"\\nLoss plot of FoodVGG using CrossEntropyLoss\")\n","    # plot_graph(\"Loss\", \"Number of Epochs\", \"Loss\", num_epochs, train_loss, val_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L50CS7DoauDh","colab_type":"text"},"source":["##Training and Hyperparameter Search"]},{"cell_type":"code","metadata":{"id":"3QmWIxm0avSn","colab_type":"code","outputId":"7bf14485-a897-41b2-add8-e6a1fb153fa7","executionInfo":{"status":"ok","timestamp":1585777862129,"user_tz":240,"elapsed":63098,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-7ot31psTGtAXzKlWBL6OEtJ7v4FOjLQdRdeG=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":218}},"source":["# batch size: 64\n","# learning rate: 0.0001\n","# number of epochs: 30\n","# number of layers: 1 fully-connected layer\n","\n","model = FoodVGG(4096)\n","\n","if torch.cuda.is_available():\n","    print(\"Using GPU...\")\n","    model = model.cuda()\n","else:\n","    print(\"Using CPU...\")\n","\n","train_vgg(model, batch_size = 64, learning_rate = 0.001, num_epochs=5)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Using CPU...\n","Loss function used: CrossEntropyLoss\n","Optimizer used: Adam\n","\n","Finished Training\n","Total time elapsed: 61.66 seconds\n","\n","-------------------------------------------------------------------------\n","Training accuracy after 5 epochs: 0.0\n","Training loss after 5 epochs: 0.0\n","\n","-------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v6lz9oiPa6uV","colab_type":"text"},"source":["##Running on Testing Dataset"]},{"cell_type":"code","metadata":{"id":"bFatsQAIa_0a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":387},"outputId":"3b4899d4-d7e8-46bb-8e7c-aa422aabfcb8","executionInfo":{"status":"error","timestamp":1585776676212,"user_tz":240,"elapsed":1177,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-7ot31psTGtAXzKlWBL6OEtJ7v4FOjLQdRdeG=s64","userId":"16176446408353667842"}}},"source":["_, _, test_loader = vgg_data_loader(batch_size = 64)\n","\n","model = FoodVGG(4096)\n","\n","if torch.cuda.is_available():\n","    print(\"Using GPU...\")\n","    model = model.cuda()\n","else:\n","    print(\"Using CPU...\")\n","\n","model_path = get_model_name(model.name, batch_size=64, learning_rate=0.001, epoch=4)\n","state = torch.load(model_path)\n","model.load_state_dict(state)\n","\n","accuracy, loss = get_accuracy_vgg(model, test_loader, nn.CrossEntropyLoss())\n","\n","print(\"The testing accuracy is:\", accuracy)\n","print(\"The testing loss is:\", loss)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Using CPU...\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-e494d1254720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_FoodVGG_bs64_lr0.001_epoch4'"]}]}]}